schema: study/v1
study_name: diffusion_10k
out_dir: "runs/"
seed: 1077
metric: val/fid

_experiment:
  name: "E7-baseline-linear-full-50nfe"

device: null     # auto
deterministic: true
clean_run: false

data:
  dataset: "cifar10"
  subset: null      # full train
  batch_size: 4
  num_workers: 0
  shuffle: true

model:
  name: "unet_cifar32"
  # ... (same architecture as E1/E2/E6)

ema:
  enabled: true
  decay: 0.999      # E7 spec: stronger EMA

diffusion:
  enabled: true
  beta_schedule: "linear"   # baseline linear β
  # num_timesteps: 1000
  # ... (keep other diffusion settings as in E1)

optim:
  optimizer: adam
  lr: 1.0e-4

train:
  total_steps: 50000        # E7 spec: 50k steps
  grad_clip: 1.0
  amp: false

# evaluation cfg:
eval:
  quick: false

  grid:
    enabled: true
    every: 5000          # or 2000 maybe
    sampler: ddim
    nfe: 20
    n_samples: 36
    save_images: true

  kid:
    enabled: false
    every: 4000
    n_samples: 1024
    repeats: 3
    sampler: ddim
    nfe: 20

  fid_milestone:
    enabled: true
    every: 10000               # FID at ~10k, 20k, 30k, 40k
    run_if_kid_improved_pct: 0.0   # <-- important: ignore KID gate
    sampler: ddpm
    nfe: 50
    n_samples: 5000            # smaller than final, cheaper
    fid_stats: "stats/cifar10_inception_train.npz"
  
  final:
    enabled: true
    at_end: true
    sampler: ddpm
    nfe: 50
    n_samples: 10000              # prereg: FID@10k
    fid_stats: "stats/cifar10_inception_train.npz"


logging:
  enable: true
  backends: ["tensorboard","wandb"]
  log_every_n_steps: 100

  wandb:
    mode: online
    project: noise_sched
    run_name: "E7-linear-full-50k"
    tags: ["E7", "baseline", "linear", "50k", "50nfe"]
    notes: "E7: long-run linear β baseline, 50k steps, EMA=0.999, eval NFE=50, 10k samples."

  tensorboard:
    flush_secs: 10